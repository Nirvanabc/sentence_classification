запускать файл в оболочке python как: import tensor


constants.py -- файл с константами, встречающиеся в нескольких
  файлах. Для удобства их изменения.

get_data.py скачивает данные с кинопоиска, сохраняет в pickle
  формате. Надо переделать, в данной проге не используется
  больше pickle.

get_data2.py -- если файл был в формате pickle (у меня таких уже
  нет), то вызывай функцию shuffle_data, если текстовый формат
  (как в MR стандартном наборе), то shuffle_text_data.
  снабжает цифрой 0 (bad) или 1 (good) каждое предложение, затем
  перемешивает и сохраняет в один текстовый файл.

prepare_data.py -- имеет два назначеня:
  1. считывает словарь.
  2. next_batch, которая считывает по кускам
  поданный ей файл (уже открытый), преобразует его в
  последовательность предложений одинаковой длины, где каждое
  слово представлено вектором из словаря.

tensor.py -- реализация CNN

tensor_2.py -- различные тестовые вариации CNN

board.py -- то же, что и tensor, только не производится
  обучение и скачивание словаря. Нужен для визуализации структуры
  сети

prepared_dict -- словарь тональности 0=отриц, 1=нейтр, 2=полож

neg_MR и pos_MR -- скачанные с сайта предложения. Вместе образуют
  corpora_MR -- перемешанную и с ярлыками базу, которая, в свою
  очередь, разделена на train_MR и test_MR -- 9000 и 2000
  предложений для обучения и проверки.

new_train_MR и new_test_MR -- то же, что и без new, только без
  пунктуации и стоп-слов. (см. ниже)

del_stop_words_and_punct.py -- чистит отдельно train и test.

restored_tensor.py -- восстанавливает сохранённую сеть. Для работы
с отдельными предложениями следует помнить: labels: [0,1] -- good,
[1,0] -- bad.



*TODO:
1. bag of words

2. Отдельно узнать, можно ли дополнить существующую база word2vec
новыми словами (т.к. всё-таки лексика от темы к теме отличается).
(да, см. статьи)

3. https://habrahabr.ru/post/208192/ читать про кодировки
в питоне 2-3



TO KNOW:

1. http://datascientist.one/tipichnye-oshibki-ispolzovaniya-neuralnetworks/ ошибки при использовании нс

2. https://habrahabr.ru/company/meanotek/blog/256593/ описание на
русском

3. https://arxiv.org/pdf/1408.5882v2.pdf оригинальная работа (en)

4. https://habrahabr.ru/company/wunderfund/blog/314872/ о сверт.
нейронной сети (см. макропараметры)

5. https://habrahabr.ru/company/microsoft/blog/314934/ почитать

6. http://ru.datasides.com/code/cnn-convolutional-neural-networks/ очень хорошая статья, на русском
подробно рассказано откуда какие цифры берутся.

7. http://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/
tensorboard для визуализации результатов

8. https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model и
https://stackoverflow.com/questions/46582546/what-is-the-real-use-of-savedmodelbuilder-add-meta-graph-in-tensorflow --
как сохранить и восстановить сеть

9. https://habrahabr.ru/post/305578/
хорошее описание на русском о tensorflow и tensorboard

10. http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/
и особенно http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/ --
*реализация* статьи YoonKim-а и объяснения что откуда

Для NLTK
9. когда захочешь работать с наивным классификатором
https://streamhacker.com/2010/05/10/text-classification-sentiment-analysis-naive-bayes-classifier/
и
http://text-processing.com/demo/sentiment/
и ещё
http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/



OUTPUT:
x = [n, 30, 300, 1] => (conv) {[n, 29, 1, 100], [n, 28, 1, 100], [n, 27, 1, 100]} -- если 3 вида
ядер, 2,3 и 4. Затем max_pool: [n, 1, 1, 100] * 3 => [n,1,1,300] => [n, 300]